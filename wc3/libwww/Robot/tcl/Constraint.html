<HTML>
<HEAD>
  <TITLE>Constraint Model of the W3C Mini Robot</TITLE>
</HEAD>
<BODY BGCOLOR="#ffffff" TEXT="#000000" LINK="#0000EE" VLINK="#551A8B">
<P>
<A HREF="/"><IMG BORDER="0" ALT="W3C" SRC="/Icons/WWW/w3c_home"></A>
<A href="/Library/"><IMG BORDER="0" ALT="Library" SRC="/Icons/WWW/Lib48x"></A>
<A href="../"><IMG BORDER="0" ALT="Robot" SRC="/Icons/WWW/robot48x"></A>
<H1 ALIGN="center">
  Constraint Model for libwww Webbot<BR>
  or<BR>
  <FONT color=red>A</FONT>nalyzing <FONT color=red>De</FONT>grees of
  <FONT color=red>S</FONT>eparation
</H1>
<P>
Jump to:
<UL>
  <LI>
    <A href="#Design">Design</A> issues and purpose
  <LI>
    <A href="#Rules">Rules</A> description and issuing of commands
  <LI>
    <A href="#Urls">URLs</A> and Web Elements
  <LI>
    <A href="#Functions">Function</A> description for extending the robot
  <LI>
    <A href="#Extensions">Extensions</A> to the robot, and what's next
</UL>
<H2>
  <A name="Design">Design issues and Purpose of the Robot</A>
</H2>
<P>
The W3C robot's purpose in crowling the web is not to accumulate data from
web sites, to generate a database, or an index, or to update documents, or
verify links, even though all of the functionality described above can easily
be added, without changing the basic skeleton of the robot as it is today.
<P>
The primary purpose of the robot is to understand the relations between web
pages, and not the content of the pages themselves. How many paths can lead
to a given page, what tree structures can be generated by following links
under a certain constraint, how many degrees of separation lie between any
two pages, what is the shortest path to reach a site, and how can intelligent
navigation be automated.
<P>
The robot will also be used as a test tool interacting with the w3c-libwww-99.
The robot code is written in Tcl, the graphic interface to the user is written
in Tk, and the library code used to provide all the network functionality
is written in C. All functionality is therefore portable in almost any system
with network access.
<P>
The most elementary web robot can fetch a web document, extract the links
from that document, follow the urls, fetch the documents, and restart for
every document. The leads to an exponentially growing number of documents
to follow every time you reach a new depth. Therefore, for a robot to be
of any use, we need a way of specifying a <STRONG>Constraint Model</STRONG>
that will allow the robot to choose which documents to follow and which not
to.
<P>
The Constraint Model is basically a set of rules, that the user specifies,
or that the program itself can formulate, to follow a crawling model specified
by the user at the beginning, or during the crawling. The next section describes
the consituent blocks of the rules (or Rule Atoms) and the ways to combine
those rules.
<H2>
  <A name="Rules">Rule Description</A>
</H2>
<P>
A rule can be simple or complex. Complex rules can be expressed as logical
combinations of simpler rules, called Rule Atoms.
<H3>
  <A name="Rules.A">Rule Atoms</A>
</H3>
<H4>
  <A name="Rules.A.1">Definition</A>
</H4>
<P>
A Rule Atom is the simplest form of a rule, and the building block of every
rule. Anything simpler can not be called a rule, and anything more complicated
cannot be called a rule atom.
<H4>
  <A name="Rules.A.2">Examples</A>
</H4>
<DL>
  <DT>
    <TT>+url:*.mit.edu/*</TT>
  <DD>
    keep documents whose url matches *.mit.edu/*
  <DD>
    matches: http://web.mit.edu/manoli/www/ and http://manoland.mit.edu/ but
    not http://web.mit.edu.ca/
  <DT>
    <TT>-body:*badword*</TT>
  <DD>
    reject documents whose body contains badword
  <DT>
    <TT>+head:*meta*</TT>
  <DD>
    keep documents whose head contains the meta keyword
  <DT>
    <TT>-date:*december*</TT>
  <DD>
    reject documents whose date contains the word december
</DL>
<H4>
  <A name="Rules.A.3">Understanding the Rule Atoms</A>
</H4>
<P>
As you can see from those examples, each <STRONG>rule atom</STRONG> has three
elements:
<OL type=a>
  <LI>
    <A name="Rules.A.3.a">The <STRONG>category</STRONG> of the rule can be one
    of the following four, in order of priority:</A>
    <OL>
      <LI>
	<STRONG>Imprerative rules (*)</STRONG>: those documents will be followed
	first and always<BR>
	<STRONG>Direction rules (+)</STRONG>: Keep: the documents will be followed
	next if they don't match -<BR>
	<STRONG>Rejection rules (-)</STRONG>: Reject: document will not be followed,
	unless it matches an imperative rule<BR>
	<STRONG>LastResort rules (?)</STRONG>: Default: if the robot has no more
	documents matching * or +, it follows ?
    </OL>
  <LI>
    <A name="Rules.A.3.b">The <STRONG>field</STRONG> of the document to search</A>
    <OL>
      <LI>
	Sample fields are: url, head, body, date, etc...<BR>
	The program calls Url_Geturl, Url_Getbody, etc, depending on the field
	specification<BR>
	To add a new field, all you need to do is provide a function
    </OL>
  <LI>
    <A name="Rules.A.3.c">The <STRONG>pattern</STRONG> to match</A>
    <OL>
      <LI>
	For the moment, it is a glob-type pattern: * matches any combination of
	characters, ? matches one character. Later on, functionality might be added,
	to handle regular expression matching.<BR>
	<STRONG>Please Note:</STRONG> If you want to match the pattern in the middle
	of a string, you have to add an initial and final * Here are some examples,
	that you should take a look at:
	<UL>
	  <LI>
	    <TT>*.mit.edu</TT> does not match http://web.mit.edu/ since the final * is
	    not present
	  <LI>
	    <TT>*.mit.edu/*</TT> matches http://web.mit.edu/people/, ftp://www.mit.edu/pub/
	  <LI>
	    <TT>*.edu*</TT> matches http://www.sfu.edu<STRONG>.co</STRONG>/,
	    ftp://www.mit.edu.ca/pub/bin/
	</UL>
    </OL>
</OL>
<H3>
  <A name="Rules.B">Combining Rules to form Rules</A>
</H3>
<H4>
  <A name="Rules.B.1">Definition of a Rule</A>
</H4>
<P>
The definition is recursive: a rule can be either a rule atom, or a logical
combination of rules.
<H4>
  <A name="Rules.B.2">Undersating Rule Combination</A>
</H4>
<P>
Here are the forms a rule can take and the result of applying the rule to
a list of documents
<TABLE BORDER CELLSPACING="0" CELLPADDING="0" WIDTH="100%">
  <TR>
    <TH align=left>Applying ... to a list of documents</TH>
    <TH align=left>Returns the list of documents that...</TH>
  </TR>
  <TR>
    <TD>RuleAtom</TD>
    <TD>match an imperative rule, or that match a directional rule and don't
      match a rejection rule</TD>
  </TR>
  <TR>
    <TD>{OR rule1 rule2 ...}</TD>
    <TD>either returned by applying rule1, or returned by applying rule2, or
      ..</TD>
  </TR>
  <TR>
    <TD>{AND rule1 rule2 ...}</TD>
    <TD>urls returned by rule1, and by rule2, and by...</TD>
  </TR>
  <TR>
    <TD>{NOT rule1 rule2}</TD>
    <TD>urls returned by rule1 and not by rule2.</TD>
  </TR>
</TABLE>
<H4>
  <A name="Rules.B.4">Example</A>
</H4>
<P>
With those simple rules, we can express very precise constraints
<PRE>{NOT {AND +url:*.mit.edu/* -url:*.mit.edu/personnal/* 
          {OR -body:*badword* +head:*goodcontext*}} 
     +date:*october*}
</PRE>
<H3>
  <A name="Rules.C">Applying Rules</A>
</H3>
<H4>
  <A name="Rules.C.1">The notion of truth</A>
</H4>
<P>
Applying a rule such as +url:*.mit.edu/* to a list of elements
<P>
more...
<P>
Please note that the logical NOT operator is not defined in its common
interpretation of a unary operator, negating something that is false, to
give something true. This is because testing a rule against a set of web
elements, does not return true or false, but the list notion of true and
false, which is sublists. Therefore, negating a list list2 cannot return
the contrary of list2, unless we know what represents the "truth" which in
this case is list1. Therefore, one must understand that we can only negate
a list relative to a larger list. {NOT 1} is always 0 and {NOT 0} is always
1, but the result of evaluating a rule against a list of documents (that
we will assimilate to URLs in this paragraph) is not a boolean, but a sublist.
Therefore, if http://web.mit.edu/ is the result of applying rule2, the {NOT
rule2} will be {NOT http://web.mit.edu/}, which would represent the entire
web, except http://web.mit.edu/. One must therefore provide the part of the
web against which http://web.mit.edu/ will be negated, and that in form of
a list.
<H4>
  <A name="Rules.C.2">Applying a rule to an Element list</A>
</H4>
<OL type=a>
  <LI>
    <A name="Rules.C.2.a">Reminder</A>
    <OL>
      <LI>
	As you saw earlier, a rule atom has the form +url:*.mit.edu/*, and is formed
	of the category specification (*, +, -, ?), the field to check (url, head,
	body, date, etc) and the pattern to match (*.mit.edu/* in this example).
    </OL>
  <LI>
    <A name="Rules.C.2.b">Applying a Rule Atom to a Web Element</A>
    <OL>
      <LI>
	Depending on the field specification of the rule, a corresponding Get$field
	function is called on the Web Element (where $field can be any of url, head,
	body, date, etc...), and the pattern is matched against that result. To be
	able to match against more fields, all one has to do is provide the code
	for getting the corresponding field from the Web Element, and the rule will
	be used with no difference. For example, to be able to use a rule +hat:top
	or +hat-color:red, all one has to do is provide the functions URL_Gethat
	and URL_Gethat-color. The name can be anything, but cannot contain a column
	(:), since it is used to separate the field from the pattern in a rule.
    </OL>
  <LI>
    <A name="Rules.C.2.c">Applying a Rule Atom to a List of Web Elements</A>
    <OL>
      <LI>
	When the Web Atom is applied to a List of Elements, two local lists are created,
	named matched, and unmatched, and each Element checked goes to one list or
	the other. After every element in the list has been checked, the result (the
	return value) of applying the Rule to the Element List is the list of matched
	elements in the case of an imperative (*) or directional (+) rule atom, or
	the list of unmatched elements, in the case of a negational (-) rule.
    </OL>
  <LI>
    <A name="Rules.C.2.d">Applying Rule Combinations to Element Lists</A>
    <OL>
      <LI>
	When applying {AND rule1 rule2 {OR rule3 rule4}} to a list of elements, rule1,
	rule2, rule3, and rule4 are applied separately to the list of elements, yielding
	list1, list2, list3, list4. Then is evaluated the expression [List_and list1
	list2 [List_or list3 list4]], which returns the desired list value, the result
	of applying the rule combination to the element list.
    </OL>
  <LI>
    <A name="Rules.C.2.e">Accepting or rejecting an element</A>
</OL>
<H3>
  <A name="Rules.D">Sets of Rules</A>
</H3>
<H4>
  <A name="Rules.D.1">Global Rules and Local Rules</A>
</H4>
<OL type=a>
  <LI>
    <A name="Rules.D.1.a">Design issues and purpose</A>
    <OL>
      <LI>
	The Constraint Model of the Robot was built on the idea that the links a
	user will choose to follow, do not only depend on the purpose of his navigation
	and his choices of sites, but also, on the history of his navigation, the
	documents accessed previous to the current page. Therefore, a constraint
	model would be more efficient by having not only a set of static, global
	rules, that are set by the user when the navigation begins, but also a set
	of local rules for each page, that are set automatically, as the navigation
	progresses, and that are specific to a page, or a family of pages.
    </OL>
  <LI>
    <A name="Rules.D.1.b">Global Rules and Constraints</A>
    <OL>
      <LI>
	Global rules are set by the user before the navigation begins. They represent
	the rules that the program has to follow throughout his navigation, and cannot
	be overwritten by local rules. For example, if a user doesn't want to access
	personnal pages, a global rule would be -url:*/Personnal/*, or if he doesn't
	want to access only educational pages, a rule might be +url:*.edu*
    </OL>
  <LI>
    <A name="Rules.D.1.c">Local Rules and Navigation method</A>
    <OL>
      <LI>
	The local rules are set every time a new element is created. They are set
	based on a global variable, nav, that the user sets at the beginning of the
	navigation, and that he can change at any time. This variable can now have
	four values: dumb, deep, broad, strategic.
    </OL>
</OL>
<H2>
  <A name="Urls">Representing Documents and Urls</A>
</H2>
<H3>
  <A name="Urls.A">A Web Element</A>
</H3>
<H4>
  <A name="Urls.A.1">Definition</A>
</H4>
<P>
A Web Element is the sum of all the information gathered on a certain web
document reached following a certain path, with a known set of rules.
<H4>
  <A name="Urls.A.2">Representation of a Web Element</A>
</H4>
<P>
A Web Element consists of:
<DL>
  <DT>
    <A name="Urls.A.2.a">Family</A>
  <DD>
    The documents reached from parent 0302 will have family indexes 030201, 030202,
    030203. The family index is unique, and fully identifies a given Element.
  <DT>
    <A name="Urls.A.2.b">Url</A>
  <DD>
    The same document can be reached from two different paths. The same url therefore
    can be shared by 030202 and 0401.
  <DT>
    <A name="Urls.A.2.c">Priority</A>
  <DD>
    It shows how important a certain element is for the navigation of the robot.
    Here's a table of values:
    <TABLE BORDER CELLSPACING="0">
      <TR>
	<TH>Value</TH>
	<TH>Category</TH>
	<TH>Corresponding rule</TH>
	<TH>Description</TH>
      </TR>
      <TR>
	<TD align=right><P ALIGN=Center>
	  0</TD>
	<TD>Untested</TD>
	<TD>none</TD>
	<TD>Element has not been tested yet</TD>
      </TR>
      <TR>
	<TD align=right><P ALIGN=Center>
	  1</TD>
	<TD>First</TD>
	<TD>imperative</TD>
	<TD>Will be followed first</TD>
      </TR>
      <TR>
	<TD align=right><P ALIGN=Center>
	  2</TD>
	<TD>Next</TD>
	<TD>directional</TD>
	<TD>Followed next</TD>
      </TR>
      <TR>
	<TD align=right><P ALIGN=Center>
	  3</TD>
	<TD>Last</TD>
	<TD>lastresort</TD>
	<TD>Followed last</TD>
      </TR>
      <TR>
	<TD align=right><P ALIGN=Center>
	  5</TD>
	<TD>Rejected</TD>
	<TD>rejection</TD>
	<TD>Never followed</TD>
      </TR>
    </TABLE>
  <DT>
    <A name="Urls.A.2.d">Rules</A>
  <DD>
    The local rules of the url. Updated automatically.
  <DT>
    <A name="Urls.A.2.e">State</A>
  <DD>
    Has the url been followed yet. 1 for yes, 0 for no.
</DL>
<H4>
  <A name="Urls.B">The Web</A>
</H4>
<P>
As was said earlier, the main interest of the robot, is not how the web is
constructed, how a site is managed, or how the web should be ordered, but
the relations that exist among documents,
<H2>
  <A name="Functions">Program Functions</A>
</H2>
<H3>
  <A name="Functions.A">Rule Functions</A>
</H3>
<P>
<TABLE BORDER CELLSPACING="0">
  <TR>
    <TH>function</TH>
    <TH>input</TH>
    <TH>outputdescription</TH>
  </TR>
  <TR>
    <TD>Rule_Init</TD>
    <TD>&nbsp;</TD>
    <TD>Initializes rules</TD>
  </TR>
  <TR>
    <TD>Rule_ShowAll</TD>
    <TD>&nbsp;</TD>
    <TD>&nbsp;</TD>
  </TR>
  <TR>
    <TD>Rule_ApplyAtom</TD>
    <TD>elementlist ruleatom</TD>
    <TD>list of matching elements</TD>
  </TR>
  <TR>
    <TD>Rule_Apply</TD>
    <TD>elementlist rule</TD>
    <TD>&nbsp;</TD>
  </TR>
  <TR>
    <TD>Rule_and</TD>
    <TD>&nbsp;</TD>
    <TD>&nbsp;</TD>
  </TR>
  <TR>
    <TD>Rule_or</TD>
    <TD>&nbsp;</TD>
    <TD>&nbsp;</TD>
  </TR>
</TABLE>
<H3>
  <A name="Functions.B">Handling Urls</A>
</H3>
<P>
<TABLE BORDER CELLSPACING="0">
  <TR>
    <TH>function</TH>
    <TH>input</TH>
    <TH>outputdescription</TH>
  </TR>
  <TR>
    <TD>Url_Init</TD>
    <TD>&nbsp;</TD>
    <TD>Initializes urls</TD>
  </TR>
  <TR>
    <TD>Url_ShowAll</TD>
    <TD>&nbsp;</TD>
    <TD>&nbsp;</TD>
  </TR>
  <TR>
    <TD>Url_ApplyAtom</TD>
    <TD>elementlist ruleatom</TD>
    <TD>list of matching elements</TD>
  </TR>
  <TR>
    <TD>Url_Apply</TD>
    <TD>elementlist rule</TD>
    <TD></TD>
  </TR>
</TABLE>
<H3>
  <A name="Functions.C">Handling Lists</A>
</H3>
<P>
<TABLE BORDER CELLSPACING="0">
  <TR>
    <TH>function</TH>
    <TH>input</TH>
    <TH>outputdescription</TH>
  </TR>
  <TR>
    <TD>List_and</TD>
    <TD>list1 list2 ...</TD>
    <TD>list of common elements to list1 list2 ...</TD>
  </TR>
  <TR>
    <TD>List_or</TD>
    <TD>list1 list2 ...</TD>
    <TD>list of elements either in list1 or list2 ...</TD>
  </TR>
  <TR>
    <TD>List_not</TD>
    <TD>list1 list2</TD>
    <TD>list of elements in list1 and not in list2</TD>
  </TR>
</TABLE>
<H2>
  <A name="Extensions">Extending the Robot</A>
</H2>
<DL>
  <DT>
    <A name="Extensions.A">Gathering more information</A>
  <DD>
    What is really easy to do with the current design of the robot, is to make
    it gather more information from the documents that it accesses. All one has
    to do is change the Url_Load function.
  <DT>
    <A name="Extensions.B">Graphical Representation</A>
  <DD>
    Once elements are gathered by the robot, their family indexes describe their
    relations very precisely. One can then choose a representation for trees,
    and provide a graphical representation of the portion of the web accessed.
  <DT>
    <A name="Extensions.C">Distributing Web indexing and crawling</A>
  <DD>
    When a stable design of storing results is reached and proven general and
    extensible, then different parties can run their robot, and create tree
    structures inside their own sites, and store these results in an agreed-upon
    location, that will be searched by the robot, when accessing that site, and
    the results will be reused.
  <DT>
    <A name="Extensions.D">Strategic navigation</A>
  <DD>
    The present robot doesn't know how to handle strategic navigation, which
    would be trying to reach a url. A special way of calculating rules will have
    to be formulated.
  <DT>
    <A name="Extensions.E">More</A>
  <DD>
    And much much more...
</DL>
<P>
  <HR>
<ADDRESS>
  Manolis Kamvysselis,<BR>
  @(#) $Id$
</ADDRESS>
</BODY></HTML>
